{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport pandas as pd \nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom rdkit.Chem import Descriptors\nfrom rdkit.ML.Descriptors import MoleculeDescriptors\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn import datasets, ensemble\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split\nfrom rdkit.Chem import Draw\nfrom rdkit import DataStructs\nfrom rdkit.Chem import AllChem\nfrom rdkit import Chem\nfrom rdkit import DataStructs\nfrom rdkit.Chem import rdFingerprintGenerator\nfrom rdkit.Chem import PandasTools\nfrom rdkit.Chem.Draw import IPythonConsole\n\nfrom IPython.display import SVG\nIPythonConsole.ipython_useSVG=True\n%matplotlib inline ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Проект по предсказанию характеристики связывания малых молекул с лекарственной мишенью. (Связываемость с рецепторами дофамина)","metadata":{}},{"cell_type":"markdown","source":"https://www.kaggle.com/datasets/bhawakshi/pec50-prediction-dopamine-receptor?resource=download&select=enriched_dopamine_ec50.csv  \nОтсюда был взял датасет","metadata":{}},{"cell_type":"markdown","source":"Состоит из четрых частей: \n1) Построение регресионной модели основанной на отпечатках моргана для предсказания значений pEC50\n2) Построение регресионной модели основанной на дескрипторах RDKit для того же предсказания (без и с работой с полученными дескрипторами)\n3) Построение классификатора по типам допаминовых рецепторов (Target Name) на отпечатках моргана\n4) Построение классификатора основанного на дескрипторах RDKit \n","metadata":{}},{"cell_type":"markdown","source":"# Подготовка датасета ","metadata":{}},{"cell_type":"markdown","source":"Экспериментальные данные: Значения EC50 (нМ), значения pEC50 (-lg(EC50)).\n\nТип анализа(Assay Type) и название мишени (Target Name): Воспроизводимость и подтип рецептора - D1, D2, D3, D4 и D5.\n\nLogP (липофильность): Указывает на гидрофобность.\n\nH_Donors и H_Acceptors: Показатели способности к водородным связям.\n\nTPSA (топологическая площадь полярной поверхности): Важна для биодоступности.\n\nКоличество колец (Ring_Count) и количество вращающихся связей (Rotatable_Bonds): Показатели сложности молекулы.","metadata":{}},{"cell_type":"markdown","source":"EC50 - или полумаксимальная эффективная концентрация, означает концентрацию лиганда, которая вызывает эффект, равный половине максимального возможного для данного лиганда после истечения некоторого промежутка времени. ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('enriched_dopamine_ec50.csv')\ndf = df.drop(['ChEMBL ID'],axis=1)\ndf","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Посмотрим на распределение потенциального таргета:","metadata":{}},{"cell_type":"code","source":"df['pEC50'].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ax = plt.hist(df['pEC50'], bins=100)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pylab \nimport scipy.stats as stats\nstats.probplot(df['pEC50'], dist=\"norm\", plot=pylab)\npylab.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Как видно из гистограммы выше, значения pEC50 = 5 и 4 являются довольно очевидными выбросами, что довольно просто объясняется тем, что в статьях значение 5 часто выбирают в качестве порога, то есть приближения, оно не отражает действительное значение pEC50\nqq plot Также показывает, что наш будущих таргет распрелен не нормально и есть выбросы","metadata":{}},{"cell_type":"code","source":"df_u = df[(df['pEC50']!=5) & (df['pEC50']!=4)]\ndf_u","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ax = plt.hist(df_u['pEC50'], bins=100)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Дополнитель проведем квартильную фильтрацию","metadata":{}},{"cell_type":"code","source":"Q1 = df_u['pEC50'].quantile(0.2)\nQ3 = df_u['pEC50'].quantile(0.8)\nIQR = Q3 - Q1\nfiltered_df =  df_u[~((df_u['pEC50'] < (Q1 - 1.5 * IQR)) |(df_u['pEC50'] > (Q3 + 1.5 * IQR)))]\nfiltered_df","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Построим qq plot чтобы сравнить наше распределение с нормальным более наглядно:","metadata":{}},{"cell_type":"code","source":"import pylab \nimport scipy.stats as stats\nstats.probplot(filtered_df['pEC50'], dist=\"norm\", plot=pylab)\npylab.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Распределение теперь выглядит хорошо, можно работать дальше","metadata":{}},{"cell_type":"code","source":"df_work = filtered_df[['SMILES','EC50 (nM)','pEC50']]\ndf_work","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Посмотрим на распределение таргета в случае -лог() и простой формы:","metadata":{}},{"cell_type":"code","source":"ax = plt.hist(df_work['EC50 (nM)'], bins=50)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ax = plt.hist(df_work['pEC50'], bins=20)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Очевидно, что распределение таргета в случае отрицательного логаримфа гораздо ближе к нормальному, будем предсказывать именно его (переменную pEC50)","metadata":{}},{"cell_type":"code","source":"df_work = df_work.drop('EC50 (nM)',axis=1)\ndf_work","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Часть 1 ","metadata":{}},{"cell_type":"markdown","source":"# Добавление отпечатков в рабочий сет","metadata":{}},{"cell_type":"markdown","source":"На данный момент стоит выбрать оптимальное значение радиуса и битности:\nДля нашей задачи подойдет перебор радиусов в диапазоне от 1 до 3, и битности  512 1024 и 2048.","metadata":{}},{"cell_type":"markdown","source":"На всякий случай канонизируем смайлз","metadata":{}},{"cell_type":"code","source":"df_work['SMILES'] = df_work['SMILES'].apply(lambda x: Chem.CanonSmiles(x))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Добавим столбец с мол-файлами","metadata":{}},{"cell_type":"code","source":"PandasTools.AddMoleculeColumnToFrame(df_work, smilesCol='SMILES', molCol='Molecule')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_work","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Добавим разные  отпечатки пальцев","metadata":{}},{"cell_type":"code","source":"def fpgen(x,n,size):\n    mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=n,fpSize=size,includeChirality=True)\n    e = mfpgen.GetFingerprint(x)#получаем FP\n    return (e) ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"r = [1,2,3]\nbit = [512,1024,2048]\nfor i in r:\n    for b in bit:\n        df_work[f'fp{i},{b}'] = df_work['Molecule'].apply(lambda x: fpgen(x,i,b))\ndf_work","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Я выберу оптимальное значение радуиса и битности путем использования RandomForestRegressor как показательной модели, после чего уже проведу побор гиперпараметров ","metadata":{}},{"cell_type":"markdown","source":"Разбиваем сет на трейн и тест с самого начала","metadata":{}},{"cell_type":"code","source":"X = df_work.drop(['SMILES','pEC50','Molecule'],axis=1)\ny = df_work['pEC50']\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=13)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = {\n    \"n_estimators\": 100,\n    \"max_depth\": 15,\n    \"min_samples_split\": 2,'min_samples_leaf': 1 \n,'random_state':13}","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Здесь я включил режим программиста и решил написать код который будет перебирать все типы отпечатков и смотреть эффективность регресионной модели добавляя в конечный список значение r2 и название столбца (конечно я мог 9 раз запустить код, но это было бы не так круто :)","metadata":{}},{"cell_type":"code","source":"col = X_train.columns\nr2_l = []\nfor c in col:\n    e = np.ones(shape=(512,))\n    e2= np.ones(shape=(1024,))\n    e3= np.ones(shape=(2048,))\n    d = np.ones(shape=(512,))\n    d2= np.ones(shape=(1024,))\n    d3= np.ones(shape=(2048,))    \n    \n    for fp in X_train[c]:\n        if len(np.array(fp)) == 512:\n            e = np.vstack((e,np.array(fp)))\n        elif len(np.array(fp)) == 1024:\n            e2 = np.vstack((e2,np.array(fp)))\n        elif len(np.array(fp)) == 2048:\n            e3 = np.vstack((e3,np.array(fp)))\n    \n    if  e.shape != (512,):\n        e = np.delete(e,0,0)\n        Train = e\n    elif e2.shape != (1024,):\n        e2 = np.delete(e2,0,0)\n        Train = e2\n    elif e3.shape != (2048,):\n        e3 = np.delete(e3,0,0)\n        Train = e3\n\n    for fp in X_test[c]:\n        if len(np.array(fp)) == 512:\n            d = np.vstack((d,np.array(fp)))\n        elif len(np.array(fp)) == 1024:\n            d2 = np.vstack((d2,np.array(fp)))\n        else:\n            d3 = np.vstack((d3,np.array(fp)))\n    \n    if  d.shape != (512,):\n        d = np.delete(d,0,0)        \n        Test = d\n    elif d2.shape != (1024,):\n        d2 = np.delete(d2,0,0)         \n        Test = d2\n    elif d3.shape != (2048,):\n        d3 = np.delete(d3,0,0)         \n        Test = d3 \n    reg = RandomForestRegressor(**params).fit(Train, y_train)\n    r2 = r2_score(y_test, reg.predict(Test))\n    r2_l.append([c,r2])\n    print(r2_l[-1]) \n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Из выведенной выше таблицы можно сделать вывод, что оптимальный выбор - это радиус = 2, и битность 2048. Но учитывая значение результата R2 можно начать подозревать, что химическая структура молекулы не является очень хорошим признаком для определения pEC50. \nНо далее мы проверим несколько различных регрисионных моделей, а так же проведем оптимизацию их гиперпараметров","metadata":{}},{"cell_type":"code","source":"d = np.ones(shape=(2048,))\nfor fp in X_test['fp2,2048']:\n    d = np.vstack((d,np.array(fp)))\nd = np.delete(d,0,0)        \nTest = d\nd = np.ones(shape=(2048,))\nfor fp in X_train['fp2,2048']:\n    d = np.vstack((d,np.array(fp)))\nd = np.delete(d,0,0)     \nTrain = d\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Проведем оптимизацию гиперпараметров для RFR","metadata":{}},{"cell_type":"markdown","source":"Сначала воспользуемся RandomizedSearchCV а потом уже GridSearchem","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nrfr = RandomForestRegressor()\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\nmax_features = ['log2', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(start = 1, stop = 15, num = 15)]\nmin_samples_split = [int(x) for x in np.linspace(start = 2, stop = 50, num = 10)]\nmin_samples_leaf = [int(x) for x in np.linspace(start = 2, stop = 50, num = 10)]\nbootstrap = [True, False]\nparam_dist = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nrs = RandomizedSearchCV(rfr, \n                        param_dist, \n                        n_iter = 100, \n                        cv = 5, \n                        verbose = 1, \n                        n_jobs=-1, \n                        random_state=0)\nrs.fit(Train, y_train)\nrs.best_params_","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"После чего строю графики для каждого параметра, где можно будет визуально увидеть лучшие значения","metadata":{}},{"cell_type":"code","source":"rs_df = pd.DataFrame(rs.cv_results_).sort_values('rank_test_score').reset_index(drop=True)\nrs_df = rs_df.drop([\n            'mean_fit_time', \n            'std_fit_time', \n            'mean_score_time',\n            'std_score_time', \n            'params', \n            'split0_test_score', \n            'split1_test_score', \n            'split2_test_score', \n            'std_test_score'],\n            axis=1)\nfig, axs = plt.subplots(ncols=3, nrows=2)\nsns.set(style=\"whitegrid\", color_codes=True, font_scale = 2)\nfig.set_size_inches(30,25)\nsns.barplot( data=rs_df,x='param_n_estimators', y='mean_test_score', ax=axs[0,0], color='lightgrey')\naxs[0,0].set_ylim([0,0.4])\naxs[0,0].set_title(label = 'n_estimators', size=30, weight='bold')\nsns.barplot(x='param_min_samples_split', y='mean_test_score', data=rs_df, ax=axs[0,1], color='coral')\naxs[0,1].set_ylim([0,0.4])\naxs[0,1].set_title(label = 'min_samples_split', size=30, weight='bold')\nsns.barplot(x='param_min_samples_leaf', y='mean_test_score', data=rs_df, ax=axs[0,2], color='lightgreen')\naxs[0,2].set_ylim([0,0.4])\naxs[0,2].set_title(label = 'min_samples_leaf', size=30, weight='bold')\nsns.barplot(x='param_max_features', y='mean_test_score', data=rs_df, ax=axs[1,0], color='wheat')\naxs[1,0].set_ylim([0,0.4])\naxs[1,0].set_title(label = 'max_features', size=30, weight='bold')\nsns.barplot(x='param_max_depth', y='mean_test_score', data=rs_df, ax=axs[1,1], color='lightpink')\naxs[1,1].set_ylim([0,0.4])\naxs[1,1].set_title(label = 'max_depth', size=30, weight='bold')\nsns.barplot(x='param_bootstrap',y='mean_test_score', data=rs_df, ax=axs[1,2], color='skyblue')\naxs[1,2].set_ylim([0,0.4])\naxs[1,2].set_title(label = 'bootstrap', size=30, weight='bold')\nplt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Далее уже из вручную выбранных значений, с помощью GridSearchCV находим наилучшую комбинацию гиперпараметров для нашей модели","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nn_estimators = [500,700]\nmin_samples_split = [7,23]\nmin_samples_leaf = [2,18]\nmax_features = ['sqrt']\nmax_depth = [6,13,15]\nbootstrap = [False,True]\nparam_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\ngs = GridSearchCV(rfr, param_grid, cv = 5, verbose = 1, n_jobs=-1)\ngs.fit(Train, y_train)\nrfc_3 = gs.best_estimator_\ngs.best_params_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = gs.best_estimator_.predict(Test)\nprint('R2 score:', r2_score(y_test,y_pred))\nprint('Mean squared error:',mean_squared_error(y_test,y_pred))\nprint('Mean absolute error:',mean_absolute_error(y_test,y_pred))\nprint('Mean absolute percentage error:',mean_absolute_percentage_error(y_test,y_pred))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Модель объясняет 32% дисперсии y, а так же в принципе дает не такой уж плохой результат (далеко не хороший, но не ужасный). При предсказании мы в среднем ошибаемся на 11 процентов.","metadata":{}},{"cell_type":"markdown","source":"# Попробуем понизить размерность отпечатков с помощью PCA","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca_test = PCA(n_components=2000)\npca_test.fit(Train)\nsns.set(style='whitegrid')\nplt.plot(np.cumsum(pca_test.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.axvline(linewidth=4, color='r', linestyle = '--', x=250, ymin=0, ymax=1)\ndisplay(plt.show())\nevr = pca_test.explained_variance_ratio_\ncvr = np.cumsum(pca_test.explained_variance_ratio_)\npca_df = pd.DataFrame()\npca_df['Cumulative Variance Ratio'] = cvr\npca_df['Explained Variance Ratio'] = evr\ndisplay(pca_df.iloc[240:251])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Взяв 250 главных компонент мы объясняем 95% дисперсии данных, попробуем посмотреть на результаты предсказаний","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=250)\npca.fit(Train)\nTrain_pca = pca.transform(Train)\npca.fit(Test)\nTest_pca = pca.transform(Test)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_estimators = [500,700]\nmin_samples_split = [7,23]\nmin_samples_leaf = [2,18]\nmax_features = ['sqrt']\nmax_depth = [6,13,15]\nbootstrap = [False,True]\nparam_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\ngs = GridSearchCV(rfr, param_grid, cv = 5, verbose = 1, n_jobs=-1)\ngs.fit(Train_pca, y_train)\ngs.best_params_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = gs.best_estimator_.predict(Test_pca)\nprint('R2 score:', r2_score(y_test,y_pred))\nprint('Mean squared error:',mean_squared_error(y_test,y_pred))\nprint('Mean absolute error:',mean_absolute_error(y_test,y_pred))\nprint('Mean absolute percentage error:',mean_absolute_percentage_error(y_test,y_pred))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Все стало еще грустнее, но это неудивительно. Применение PCA на отпечатки не несет большого смысла в силу того, что уменьшая размерность путем проецирования на гиперплоскость нужной размерности мы теряем структуру отпечатков, где нужные нам фрагменты описывались единичками. Мы мало того, что теряем объясненную дисперсию, но еще и сильно меняем структуру фичей не в лучшую сторону","metadata":{}},{"cell_type":"markdown","source":"# Попробуем градиентный бустинг..","metadata":{}},{"cell_type":"code","source":"reg = ensemble.GradientBoostingRegressor()\nn_estimators = [500]\nmin_samples_split = [7]\nmin_samples_leaf = [10]\nmax_depth = [13]\nparam_grid = {'n_estimators': n_estimators,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\ngs = GridSearchCV(reg, param_grid, cv = 5, verbose = 1, n_jobs=-1)\ngs.fit(Train, y_train)\ngs.best_params_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = gs.best_estimator_.predict(Test)\nprint('R2 score:', r2_score(y_test,y_pred))\nprint('Mean squared error:',mean_squared_error(y_test,y_pred))\nprint('Mean absolute error:',mean_absolute_error(y_test,y_pred))\nprint('Mean absolute percentage error:',mean_absolute_percentage_error(y_test,y_pred))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Градиентый бустинг для данной задачи показывает себя ощутимо хуже по R2, но чуть лучше по MAE","metadata":{}},{"cell_type":"markdown","source":"# Перестаем мучить отпечатки и воспользуемся дескрипторами RDKit","metadata":{}},{"cell_type":"markdown","source":"# Часть вторая","metadata":{}},{"cell_type":"markdown","source":"Из молфайлов получим набор всех дескрипторов","metadata":{}},{"cell_type":"code","source":"df_desc = df_work[['SMILES','pEC50']]\ndf_desc = df_desc.reset_index()\ndf_desc","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def RDkit_descriptors(smiles):\n    mols = [Chem.MolFromSmiles(i) for i in smiles] \n    calc = MoleculeDescriptors.MolecularDescriptorCalculator([x[0] for x in Descriptors._descList])\n    desc_names = calc.GetDescriptorNames()\n    \n    Mol_descriptors =[]\n    for mol in mols:\n        # add hydrogens to molecules\n        mol=Chem.AddHs(mol)\n        # Calculate all 200 descriptors for each molecule\n        descriptors = calc.CalcDescriptors(mol)\n        Mol_descriptors.append(descriptors)\n    return Mol_descriptors,desc_names ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Mol_descriptors,desc_names = RDkit_descriptors(df_desc['SMILES'])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_with_descriptors = pd.DataFrame(Mol_descriptors,columns=desc_names)\ndf_with_descriptors","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_with_descriptors.describe()","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"raw","source":"Приведем все распределения к виду стандартного нормального, насколько это возможно","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(df_with_descriptors)\nscaled_df = pd.DataFrame(scaler.transform(df_with_descriptors), columns = df_with_descriptors.columns)\nscaled_df['pEC50'] = df_desc['pEC50']\nscaled_df","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaled_df.isnull().any(axis=1).sum() # у нас есть 15 строк с пропусками","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaled_df.isnull().sum().sum()#суммарно у нас пропущено 180 значениф","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Найдем колонки с пропусками","metadata":{}},{"cell_type":"code","source":"a = scaled_df.loc[:, scaled_df.isnull().any()]\na","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Найдем строки среди данных колонок с пропусками","metadata":{}},{"cell_type":"code","source":"a[a.isnull().any(axis=1)]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Если мы просто выбросим все данные строки, то мы потеряем 15 строк из 2858, что составляет половину процента от всей базы.\nМне кажется, что дозаполнять данный датасет не имеет смысла. Избавимся от строк с пустыми значениями","metadata":{}},{"cell_type":"code","source":"scaled_df = scaled_df.dropna()\nscaled_df","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"С помощью метода IsolationForest мы можем избавиться от того, что данная модель посчитает выбросами, что может быть полезно для нас","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\ndata_imputed=scaled_df\niforest = IsolationForest(n_estimators = 500, random_state=0,\n    max_samples = \"auto\", \n    contamination= 0.05 #зададим сами процент потенциальных аномалий в БД\n                          #Иначе алгоритм попытается определить это самостоятельно, иногда удаляя слишком много данных\n    )\niforest_fit = iforest.fit(data_imputed)\npredictions = iforest_fit.predict(data_imputed)\n\ndata_imputed['is_anomaly_prediction'] = predictions\ndata_imputed = data_imputed[data_imputed.is_anomaly_prediction != -1]\ndata_imputed = data_imputed.drop(columns = ['is_anomaly_prediction'])\ndata_imputed.describe()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"У нас осталось 2700 сэмплов из 2858 исходных.","metadata":{}},{"cell_type":"markdown","source":"# Посмотрим на кореляцию наших признаков","metadata":{}},{"cell_type":"code","source":"sp_corr_mat = data_imputed.corr(method = 'spearman')\nsp_corr_mat","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = plt.figure(figsize=(5, 3))\nplt.title(\"Spearman Correlation Matrix\")\nmask = np.triu(np.ones_like(sp_corr_mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(sp_corr_mat,\n           mask = mask,\n           cmap=\"Blues\", annot=True, center=0, fmt='.2f')\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Выше мы можем на глаз видеть много различных признаков, которые сильно коррелируют. Но при таком количестве признаков на глаз найти все пары и убрать ненужные фичи практически невозможно\n\nПоэтому воспользуемся функцией:\nс ее помощью мы можем получить список списков пар всех фичей, корреляция которых превышает 0.6","metadata":{}},{"cell_type":"code","source":"def print_highly_correlated(df, features, threshold=0.6):\n    \"\"\"Prints highly correlated features pairs in the data frame (helpful for feature engineering)\"\"\"\n    corr_df = df[features].corr(method = 'spearman') # get correlations\n    correlated_features = np.where(np.abs(corr_df) > threshold) # select ones above the abs threshold\n    correlated_features = [(corr_df.iloc[x,y], x, y) for x, y in zip(*correlated_features) if x != y and x < y] # avoid duplication\n    s_corr_list = sorted(correlated_features, key=lambda x: -abs(x[0])) # sort by correlation value\n    g = []\n    if s_corr_list == []:\n        print(\"There are no highly correlated features with correlation above\", threshold)\n    else:\n        for v, i, j in s_corr_list:\n            cols = df[features].columns\n            g.append([corr_df.columns[j],corr_df.index[i]])\n            print (\"%s and %s = %.3f\" % (corr_df.index[i], corr_df.columns[j], v))\n    return(g)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"high_cor_pairs = print_highly_correlated(df=data_imputed, features=data_imputed.columns)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(high_cor_pairs)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Итого у нас есть 208 пар высокоскоррелированных признаков, сейчас нам нужно начать удалять признаки, но так чтобы не удалить лишние. Так как есть пересечения между различными признаками","metadata":{}},{"cell_type":"code","source":"def print_highly_correlated2(df, features, threshold=0.6):\n    \"\"\"Prints highly correlated features pairs in the data frame (helpful for feature engineering)\"\"\"\n    corr_df = df[features].corr(method = 'spearman') # get correlations\n    correlated_features = np.where(np.abs(corr_df) > threshold) # select ones above the abs threshold\n    correlated_features = [(corr_df.iloc[x,y], x, y) for x, y in zip(*correlated_features) if x != y and x < y] # avoid duplication\n    s_corr_list = sorted(correlated_features, key=lambda x: -abs(x[0])) # sort by correlation value\n    g = []\n    if s_corr_list == []:\n        print(\"There are no highly correlated features with correlation above\", threshold)\n    else:\n        for v, i, j in s_corr_list:\n            cols = df[features].columns\n            g.append([corr_df.columns[j],corr_df.index[i]])\n    return(g)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_drop = data_imputed.drop(high_cor_pairs[0][0],axis=1)\ndf_drop","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"while high_cor_pairs != []:\n    high_cor_pairs = print_highly_correlated2(df=df_drop, features=df_drop.columns)\n    if high_cor_pairs == []:\n        break\n    df_drop = df_drop.drop(high_cor_pairs[0][0],axis=1)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Теперь в нашем датасете нет ни одной пары признаков корреляция спирмана для которых >=0.6","metadata":{}},{"cell_type":"code","source":"df_drop","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sp_corr_mat = df_drop.corr(method = 'spearman')\nfig = plt.figure(figsize=(5, 3))\nplt.title(\"Spearman Correlation Matrix\")\nmask = np.triu(np.ones_like(sp_corr_mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(sp_corr_mat,\n           mask = mask,\n           cmap=\"Blues\", annot=True, center=0, fmt='.2f')\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Как видим теперь максимум на графике соответсвует 0.6, чего мы и хотели добиться :)\nцифра 0.6 появилась постфактум, потому что изначально я брал значение в 0.85, но разница между моделями была еще меньше..","metadata":{}},{"cell_type":"markdown","source":"Таким образом мы понизили размерность с 211 до 114","metadata":{}},{"cell_type":"markdown","source":"# Battle двух датасетов:","metadata":{}},{"cell_type":"markdown","source":"Дата сет со всеми посчитанными дескрипторами RDKit (**df1**) vs Очищенный от лишних фичей (по критерию корреляции Спирмана) (**df2**) ","metadata":{}},{"cell_type":"code","source":"df1 = data_imputed\ndf1","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2 = data_imputed[df_drop.columns]\ndf2","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df1.drop(['pEC50'],axis=1)\ny = df1['pEC50']\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=13)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_traindf2 = X_train[df_drop.drop(['pEC50'],axis=1).columns]\nX_testdf2 =  X_test[df_drop.drop(['pEC50'],axis=1).columns]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"У нас готовы одинаковые сэмплы, X_train (df1) и X_traindf2 (df2), теперь мы можем объективно сравнивать эффективность моделей основанных на наших датасетах","metadata":{}},{"cell_type":"code","source":"X_traindf2","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nrfr = RandomForestRegressor()\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\nmax_features = ['log2', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(start = 1, stop = 15, num = 15)]\nmin_samples_split = [int(x) for x in np.linspace(start = 2, stop = 50, num = 10)]\nmin_samples_leaf = [int(x) for x in np.linspace(start = 2, stop = 50, num = 10)]\nbootstrap = [True, False]\nparam_dist = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nrs = RandomizedSearchCV(rfr, \n                        param_dist, \n                        n_iter = 100, \n                        cv = 3, \n                        verbose = 1, \n                        n_jobs=-1, \n                        random_state=0)\nrs.fit(X_traindf2, y_train)\nrs.best_params_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rs_df = pd.DataFrame(rs.cv_results_).sort_values('rank_test_score').reset_index(drop=True)\nrs_df = rs_df.drop([\n            'mean_fit_time', \n            'std_fit_time', \n            'mean_score_time',\n            'std_score_time', \n            'params', \n            'split0_test_score', \n            'split1_test_score', \n            'split2_test_score', \n            'std_test_score'],\n            axis=1)\nfig, axs = plt.subplots(ncols=3, nrows=2)\nsns.set(style=\"whitegrid\", color_codes=True, font_scale = 2)\nfig.set_size_inches(30,25)\nsns.barplot( data=rs_df,x='param_n_estimators', y='mean_test_score', ax=axs[0,0], color='lightgrey')\naxs[0,0].set_ylim([0,0.4])\naxs[0,0].set_title(label = 'n_estimators', size=30, weight='bold')\nsns.barplot(x='param_min_samples_split', y='mean_test_score', data=rs_df, ax=axs[0,1], color='coral')\naxs[0,1].set_ylim([0,0.4])\naxs[0,1].set_title(label = 'min_samples_split', size=30, weight='bold')\nsns.barplot(x='param_min_samples_leaf', y='mean_test_score', data=rs_df, ax=axs[0,2], color='lightgreen')\naxs[0,2].set_ylim([0,0.4])\naxs[0,2].set_title(label = 'min_samples_leaf', size=30, weight='bold')\nsns.barplot(x='param_max_features', y='mean_test_score', data=rs_df, ax=axs[1,0], color='wheat')\naxs[1,0].set_ylim([0,0.4])\naxs[1,0].set_title(label = 'max_features', size=30, weight='bold')\nsns.barplot(x='param_max_depth', y='mean_test_score', data=rs_df, ax=axs[1,1], color='lightpink')\naxs[1,1].set_ylim([0,0.4])\naxs[1,1].set_title(label = 'max_depth', size=30, weight='bold')\nsns.barplot(x='param_bootstrap',y='mean_test_score', data=rs_df, ax=axs[1,2], color='skyblue')\naxs[1,2].set_ylim([0,0.4])\naxs[1,2].set_title(label = 'bootstrap', size=30, weight='bold')\nplt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_estimators = [500,700]\nmin_samples_split = [23,44]\nmin_samples_leaf = [2,18]\nmax_features = ['sqrt']\nmax_depth = [7,13,15]\nbootstrap = [True]\nparam_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\ngs = GridSearchCV(rfr, param_grid, cv = 3, verbose = 1, n_jobs=-1)\ngs.fit(X_traindf2, y_train)\nrfc_3 = gs.best_estimator_\ngs.best_params_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = gs.best_estimator_.predict(X_testdf2)\nprint('R2 score:', r2_score(y_test,y_pred))\nprint('Mean squared error:',mean_squared_error(y_test,y_pred))\nprint('Mean absolute error:',mean_absolute_error(y_test,y_pred))\nprint('Mean absolute percentage error:',mean_absolute_percentage_error(y_test,y_pred))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Результаты оказались не настолько впечатляющими, как я ожидал","metadata":{}},{"cell_type":"code","source":"rfc_3.fit(X_train, y_train)\ny_pred = rfc_3.predict(X_test)\nprint('R2 score:', r2_score(y_test,y_pred))\nprint('Mean squared error:',mean_squared_error(y_test,y_pred))\nprint('Mean absolute error:',mean_absolute_error(y_test,y_pred))\nprint('Mean absolute percentage error:',mean_absolute_percentage_error(y_test,y_pred))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Разница оказалась минимальна для RFR.......\nДа, в случае df2, где размерность ниже результат немного хуже, но это совсем минимальная разница, нооооо мы уменьшили размерность ощутимо! Не потеряв качества, это уже победа","metadata":{}},{"cell_type":"markdown","source":"Посмотрим градиентный бустинг.. (я не перебираю большое количество гиперпараметров в силу того, что мой комп итак взрывается от него..)","metadata":{}},{"cell_type":"code","source":"reg = ensemble.GradientBoostingRegressor()\nn_estimators = [500]\nmin_samples_split = [13]\nmin_samples_leaf = [10]\nmax_depth = [13]\nparam_grid = {'n_estimators': n_estimators,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\ngs = GridSearchCV(reg, param_grid, cv = 3, verbose = 1, n_jobs=-1)\ngs.fit(X_traindf2, y_train)\ngbr = gs.best_estimator_\ngs.best_params_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = gs.best_estimator_.predict(X_testdf2)\nprint('R2 score:', r2_score(y_test,y_pred))\nprint('Mean squared error:',mean_squared_error(y_test,y_pred))\nprint('Mean absolute error:',mean_absolute_error(y_test,y_pred))\nprint('Mean absolute percentage error:',mean_absolute_percentage_error(y_test,y_pred))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gbr.fit(X_train, y_train)\ny_pred = rfc_3.predict(X_test)\nprint('R2 score:', r2_score(y_test,y_pred))\nprint('Mean squared error:',mean_squared_error(y_test,y_pred))\nprint('Mean absolute error:',mean_absolute_error(y_test,y_pred))\nprint('Mean absolute percentage error:',mean_absolute_percentage_error(y_test,y_pred))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Градиентный бустинг дал другую ситуацию... но с гораздо большей разницей между сетами\n","metadata":{}},{"cell_type":"markdown","source":"# Попробуем понизить размерность с помощью PCA","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca_test = PCA(n_components=210)\npca_test.fit(X_train)\nsns.set(style='whitegrid')\nplt.plot(np.cumsum(pca_test.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.axvline(linewidth=4, color='r', linestyle = '--', x=53, ymin=0, ymax=1)\ndisplay(plt.show())\nevr = pca_test.explained_variance_ratio_\ncvr = np.cumsum(pca_test.explained_variance_ratio_)\npca_df = pd.DataFrame()\npca_df['Cumulative Variance Ratio'] = cvr\npca_df['Explained Variance Ratio'] = evr\ndisplay(pca_df.iloc[50:60])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Взяв 53 главных компонент мы объясняем 95% дисперсии данных, попробуем посмотреть на результаты предсказаний","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=53)\npca.fit(X_train)\nTrain_pca = pca.transform(X_train)\npca.fit(X_test)\nTest_pca = pca.transform(X_test)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rfc_3.fit(Train_pca,y_train)\ny_pred = rfc_3.predict(Test_pca)\nprint('R2 score:', r2_score(y_test,y_pred))\nprint('Mean squared error:',mean_squared_error(y_test,y_pred))\nprint('Mean absolute error:',mean_absolute_error(y_test,y_pred))\nprint('Mean absolute percentage error:',mean_absolute_percentage_error(y_test,y_pred))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Как видно, использование метода главных компонент на датасете со всеми дескрипторами дает ощутимо худший результат, что логично, ведь у нас крайне много высокоскоррелированных признаков, которые никуда не делись, а лишь проецировались на гиперплоскость..","metadata":{}},{"cell_type":"markdown","source":"Посмотрим результат на исходных данных из датасета, да мы разобьем данные най трейн и тест другим образом, результат нельзя будет сравнивать напрямую, но оценить порядок ошибки мы сможем  \nЗдесь я энкодю категориальную переменную, чтобы она правильно воспринималась нашими моделями","metadata":{}},{"cell_type":"code","source":"old = filtered_df.drop(['Assay Type','EC50 (nM)','SMILES'],axis=1)\ndf_encoded = pd.get_dummies(old, columns=['Target Name'], dtype = int)\ny = df_encoded['pEC50']\nX = df_encoded.drop('pEC50',axis=1)\ndf_encoded","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=13)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_estimators = [500,700]\nmin_samples_split = [23,44]\nmin_samples_leaf = [2,18]\nmax_features = ['sqrt']\nmax_depth = [7,13,15]\nbootstrap = [False,True]\nparam_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\ngs = GridSearchCV(rfr, param_grid, cv = 3, verbose = 1, n_jobs=-1)\ngs.fit(X_train, y_train)\nrfc_3 = gs.best_estimator_\ngs.best_params_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = gs.best_estimator_.predict(X_test)\nprint('R2 score:', r2_score(y_test,y_pred))\nprint('Mean squared error:',mean_squared_error(y_test,y_pred))\nprint('Mean absolute error:',mean_absolute_error(y_test,y_pred))\nprint('Mean absolute percentage error:',mean_absolute_percentage_error(y_test,y_pred))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Итого мы имеем следующие результаты для задачи регрессии:","metadata":{}},{"cell_type":"markdown","source":"В случае использования отпечатков моргана радиуса 2, битности 2048 и использования **RandomForestRegressor** в качестве модели, при оптимизации гиперпараметров мы получаем: (в схожих исследованиях в основном ориентируется на значение MAE, как ключевое)   \n**R2 score: 0.3154795876046378  \nMean absolute error: 0.7552310235089378**\n\nЕсли мы используем все то же самое, но дополнительно пользуемся **PCA** для понижения размерности мы получаем результаты ощутимо хуже:  \n**R2 score: 0.060187806504677366  \nMean absolute error: 0.9014812592919748**\n\n**GradientBoostingRegressor** для данной задачи показывает себя хуже по R2, но чуть лучше по MAE  \n**R2 score: 0.27147601901556573  \nMean absolute error: 0.7475003831849716**\n","metadata":{}},{"cell_type":"markdown","source":"И казалось, что отсюда хочется сделать вывод, что задачу QSAR для данного датасета не получится решить эффективно, и следущий логичный шаг это посмотреть на результаты использования дескрипторов RDKit.  \nТаким образом мы получили два датасета, один в котором содержатся все дискрипторы посчитанные RDKit, без какой либо доп обработки, кроме удаления NaN. Над вторым сетом уже проведена большая работа, в нем был проведен анализ корреляций признаков Спирмана и исходя их этого были удалены признаки таким образом, чтобы не осталось пар, для который корреляция спирмана превосходила бы **0.6**.  \n\nРезультаты **RFR** для датасета с дескрипторами без обработки  \n**R2 score: 0.37442611209050225  \nMean absolute error: 0.6947616846466108**  \n\nРезультаты **GBR** для датасета с дескрипторами без обработки  \n**R2 score: 0.37442611209050225  \nMean absolute error: 0.6947616846466108**  \n\nРезультаты **RFR** для датасета с обработкой высокоскоррелированных признаков:  \n**R2 score: 0.36710239313100945  \nMean absolute error: 0.7059629466610295**  \n\nРезультаты **GBR** для датасета с обработкой высокоскоррелированных признаков:   \n**R2 score: 0.21537929036797976  \nMean absolute error: 0.7338091826656238**  \n\nРезультаты **RFR** для датасета со всеми дескрипторами, но с использованием PCA:     \n**R2 score: 0.02199217071670001  \nMean absolute error: 0.9114822618885962**  \n\nТакже можно посмотреть на результаты для модели, обученной на заранее отобранных авторами датасета признаков:  \n**RFR**:  \n**R2 score: 0.3199306108855291  \nMean absolute error: 0.7261527544544696**  \n\nТаким образом с помощью дескрипторов RDKita мы смогли добиться ощутимо лучших результатов как по R2 так и по MAE\nОптимальным методом для решения данной проблемы, это использовать дескрипторы RDKita с последующим удалением ненужных высоскоррелированных фичей и в качестве модели использовать RandomForestRegressor с последующей оптимизацией гиперпараметров.\n\nВ интернете я нашел проект чувака, который на этом датасете решал схожую задачу, и невероятно, но у меня результат по MAE лучше на 0.1:)  \n\nитого часть про регрессию все...","metadata":{}},{"cell_type":"markdown","source":"# Часть 3: Задача Классификации с помощью отпечатков","metadata":{}},{"cell_type":"markdown","source":"С помощью отпечатков моргана в качестве фичей создадим модель классификации одного из 5 типов допаминовых рецепторов (Target Name)","metadata":{}},{"cell_type":"code","source":"class_df = filtered_df[['SMILES','Target Name']]\nclass_df","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"На всякий случай канонизируем смайлз","metadata":{}},{"cell_type":"code","source":"class_df['SMILES'] = class_df['SMILES'].apply(lambda x: Chem.CanonSmiles(x))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Добавим столбец с мол-файлами","metadata":{}},{"cell_type":"code","source":"PandasTools.AddMoleculeColumnToFrame(class_df, smilesCol='SMILES', molCol='Molecule')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_df","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Добавим отпечатки пальцев r = 2, битность 2048","metadata":{}},{"cell_type":"code","source":"def fpgen(x,n,size):\n    mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=n,fpSize=size,includeChirality=True)\n    e = mfpgen.GetFingerprint(x)#получаем FP\n    return (e) ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"r = [2]\nbit = [2048]\nfor i in r:\n    for b in bit:\n        class_df[f'fp{i},{b}'] = class_df['Molecule'].apply(lambda x: fpgen(x,i,b))\nclass_df","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_df['Target Name'].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Из распределения классов выше видно, что D5 слишком мало. Проще его убрать и сосредоточиться на предсказании оставшихся","metadata":{}},{"cell_type":"code","source":"class_df = class_df[class_df['Target Name'] !='Dopamine D5 receptor']","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_df","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Разбиваем сет на трейн и тест с самого начала","metadata":{}},{"cell_type":"code","source":"X = class_df.drop(['SMILES','Target Name','Molecule'],axis=1)\ny = class_df['Target Name']\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=13, stratify=y)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"d = np.ones(shape=(2048,))\nfor fp in X_test['fp2,2048']:\n    d = np.vstack((d,np.array(fp)))\nd = np.delete(d,0,0)        \nTest = d\nd = np.ones(shape=(2048,))\nfor fp in X_train['fp2,2048']:\n    d = np.vstack((d,np.array(fp)))\nd = np.delete(d,0,0)     \nTrain = d","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"У нас готов датасет с отпечатками радиуса =2 и битностью 2048 в качестве фичей","metadata":{}},{"cell_type":"markdown","source":"# Проведем оптимизацию гиперпараметров для RFC","metadata":{}},{"cell_type":"markdown","source":"Сначала воспользуемся RandomizedSearchCV а потом уже GridSearchem","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier()\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\nmax_features = ['log2', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(start = 1, stop = 15, num = 15)]\nmin_samples_split = [int(x) for x in np.linspace(start = 2, stop = 50, num = 10)]\nmin_samples_leaf = [int(x) for x in np.linspace(start = 2, stop = 50, num = 10)]\nbootstrap = [True, False]\nparam_dist = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nrs = RandomizedSearchCV(rfc, \n                        param_dist, \n                        n_iter = 100, \n                        cv = 5, \n                        verbose = 1, \n                        n_jobs=-1, \n                        random_state=0)\nrs.fit(Train, y_train)\nrs.best_params_","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"После чего строю графики для каждого параметра, где можно будет визуально увидеть лучшие значения","metadata":{}},{"cell_type":"code","source":"rs_df = pd.DataFrame(rs.cv_results_).sort_values('rank_test_score').reset_index(drop=True)\nrs_df = rs_df.drop([\n            'mean_fit_time', \n            'std_fit_time', \n            'mean_score_time',\n            'std_score_time', \n            'params', \n            'split0_test_score', \n            'split1_test_score', \n            'split2_test_score', \n            'std_test_score'],\n            axis=1)\nfig, axs = plt.subplots(ncols=3, nrows=2)\nsns.set(style=\"whitegrid\", color_codes=True, font_scale = 2)\nfig.set_size_inches(30,25)\nsns.barplot( data=rs_df,x='param_n_estimators', y='mean_test_score', ax=axs[0,0], color='lightgrey')\naxs[0,0].set_ylim([0,0.8])\naxs[0,0].set_title(label = 'n_estimators', size=30, weight='bold')\nsns.barplot(x='param_min_samples_split', y='mean_test_score', data=rs_df, ax=axs[0,1], color='coral')\naxs[0,1].set_ylim([0,0.8])\naxs[0,1].set_title(label = 'min_samples_split', size=30, weight='bold')\nsns.barplot(x='param_min_samples_leaf', y='mean_test_score', data=rs_df, ax=axs[0,2], color='lightgreen')\naxs[0,2].set_ylim([0,0.8])\naxs[0,2].set_title(label = 'min_samples_leaf', size=30, weight='bold')\nsns.barplot(x='param_max_features', y='mean_test_score', data=rs_df, ax=axs[1,0], color='wheat')\naxs[1,0].set_ylim([0,0.8])\naxs[1,0].set_title(label = 'max_features', size=30, weight='bold')\nsns.barplot(x='param_max_depth', y='mean_test_score', data=rs_df, ax=axs[1,1], color='lightpink')\naxs[1,1].set_ylim([0,0.8])\naxs[1,1].set_title(label = 'max_depth', size=30, weight='bold')\nsns.barplot(x='param_bootstrap',y='mean_test_score', data=rs_df, ax=axs[1,2], color='skyblue')\naxs[1,2].set_ylim([0,0.8])\naxs[1,2].set_title(label = 'bootstrap', size=30, weight='bold')\nplt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Далее уже из вручную выбранных значений, с помощью GridSearchCV находим наилучшую комбинацию гиперпараметров для нашей модели","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nn_estimators = [500,700]\nmin_samples_split = [2,23]\nmin_samples_leaf = [2,18]\nmax_features = ['sqrt']\nmax_depth = [6,15]\nbootstrap = [True]\nparam_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\ngs = GridSearchCV(rfc, param_grid, cv = 3, verbose = 1, n_jobs=-1)\ngs.fit(Train, y_train)\nrfc_3 = gs.best_estimator_\ngs.best_params_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = gs.best_estimator_.predict(Test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Результаты неплохие, мы в 0.76 случаев определяем правильный класс, при этом имея достоный F1-Score почти для всех классов","metadata":{}},{"cell_type":"markdown","source":"# Попробуем понизить размерность с помощью PCA","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca_test = PCA(n_components=1977)\npca_test.fit(Train)\nsns.set(style='whitegrid')\nplt.plot(np.cumsum(pca_test.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.axvline(linewidth=4, color='r', linestyle = '--', x=246, ymin=0, ymax=1)\ndisplay(plt.show())\nevr = pca_test.explained_variance_ratio_\ncvr = np.cumsum(pca_test.explained_variance_ratio_)\npca_df = pd.DataFrame()\npca_df['Cumulative Variance Ratio'] = cvr\npca_df['Explained Variance Ratio'] = evr\ndisplay(pca_df.iloc[240:250])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Взяв 246 главных компонент мы объясняем 95% дисперсии данных, попробуем посмотреть на результаты предсказаний","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=246)\npca.fit(Train)\nTrain_pca = pca.transform(Train)\npca.fit(Test)\nTest_pca = pca.transform(Test)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rfc_3.fit(Train_pca,y_train)\ny_pred = gs.best_estimator_.predict(Test_pca)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Что было понятно с самого начала, применение PCA для отпечатков не приводит к хорошим результам, так поступать не стоит!","metadata":{}},{"cell_type":"markdown","source":"# Проверим GradientBoostingClassifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = ensemble.GradientBoostingClassifier(random_state = 0)\nn_estimators = [500]\nmin_samples_split = [6]\nmin_samples_leaf = [10]\nmax_depth = [10]\nparam_grid = {'n_estimators': n_estimators,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\ngs = GridSearchCV(gbc, param_grid, cv = 3, verbose = 1, n_jobs=-1)\ngs.fit(Train, y_train)\ngbr = gs.best_estimator_\ngs.best_params_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = gs.best_estimator_.predict(Test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Градиентый бустинг оказался чуть менее точным, но зато с лучшим средним F1-score...","metadata":{}},{"cell_type":"markdown","source":"# 4 Часть: классификация с помощью дескрипторов RDKit","metadata":{}},{"cell_type":"code","source":"df_desc2 = filtered_df['Target Name']\ndf_desc2 = df_desc2.reset_index()\ndf_desc2","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ndf_desc2 = df_desc2[df_desc2['Target Name'] !='Dopamine D5 receptor']\nle = LabelEncoder()\nle.fit(df_desc2['Target Name'])\ndf_desc2['Target Name'] = le.transform(df_desc2['Target Name'])\ndf_desc2['Target Name'].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(df_with_descriptors)\nscaled_df2 = pd.DataFrame(scaler.transform(df_with_descriptors), columns = df_with_descriptors.columns)\nscaled_df2['Target Name'] = df_desc2['Target Name']\nscaled_df2 = scaled_df2.dropna()\nscaled_df2","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\ndata_imputed=scaled_df2\niforest = IsolationForest(n_estimators = 500, random_state=0,\n    max_samples = \"auto\", \n    contamination= 0.05 #зададим сами процент потенциальных аномалий в БД\n                          #Иначе алгоритм попытается определить это самостоятельно, иногда удаляя слишком много данных\n    )\niforest_fit = iforest.fit(data_imputed)\npredictions = iforest_fit.predict(data_imputed)\n\ndata_imputed['is_anomaly_prediction'] = predictions\ndata_imputed = data_imputed[data_imputed.is_anomaly_prediction != -1]\ndata_imputed = data_imputed.drop(columns = ['is_anomaly_prediction'])\ndata_imputed.describe()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Посмотрим на кореляцию наших признаков","metadata":{}},{"cell_type":"code","source":"sp_corr_mat = data_imputed.corr(method = 'spearman')\nfig = plt.figure(figsize=(5, 3))\nplt.title(\"Spearman Correlation Matrix\")\nmask = np.triu(np.ones_like(sp_corr_mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(sp_corr_mat,\n           mask = mask,\n           cmap=\"Blues\", annot=True, center=0, fmt='.2f')\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Выше мы можем на глаз видеть много различных признаков, которые сильно коррелируют. Но при таком количестве признаков на глаз найти все пары и убрать ненужные фичи практически невозможно\n\nПоэтому воспользуемся функцией:\nс ее помощью мы можем получить список списков пар всех фичей, корреляция которых превышает 0.6","metadata":{}},{"cell_type":"code","source":"def print_highly_correlated(df, features, threshold=0.6):\n    \"\"\"Prints highly correlated features pairs in the data frame (helpful for feature engineering)\"\"\"\n    corr_df = df[features].corr(method = 'spearman') # get correlations\n    correlated_features = np.where(np.abs(corr_df) > threshold) # select ones above the abs threshold\n    correlated_features = [(corr_df.iloc[x,y], x, y) for x, y in zip(*correlated_features) if x != y and x < y] # avoid duplication\n    s_corr_list = sorted(correlated_features, key=lambda x: -abs(x[0])) # sort by correlation value\n    g = []\n    if s_corr_list == []:\n        print(\"There are no highly correlated features with correlation above\", threshold)\n    else:\n        for v, i, j in s_corr_list:\n            cols = df[features].columns\n            g.append([corr_df.columns[j],corr_df.index[i]])\n            print (\"%s and %s = %.3f\" % (corr_df.index[i], corr_df.columns[j], v))\n    return(g)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"high_cor_pairs = print_highly_correlated(df=data_imputed, features=data_imputed.columns)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Итого у нас есть 700+ пар высокоскоррелированных признаков, сейчас нам нужно начать удалять признаки, но так чтобы не удалить лишние. Так как есть пересечения между различными признаками","metadata":{}},{"cell_type":"code","source":"def print_highly_correlated2(df, features, threshold=0.6):\n    \"\"\"Prints highly correlated features pairs in the data frame (helpful for feature engineering)\"\"\"\n    corr_df = df[features].corr(method = 'spearman') # get correlations\n    correlated_features = np.where(np.abs(corr_df) > threshold) # select ones above the abs threshold\n    correlated_features = [(corr_df.iloc[x,y], x, y) for x, y in zip(*correlated_features) if x != y and x < y] # avoid duplication\n    s_corr_list = sorted(correlated_features, key=lambda x: -abs(x[0])) # sort by correlation value\n    g = []\n    if s_corr_list == []:\n        print(\"There are no highly correlated features with correlation above\", threshold)\n    else:\n        for v, i, j in s_corr_list:\n            cols = df[features].columns\n            g.append([corr_df.columns[j],corr_df.index[i]])\n    return(g)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_drop = data_imputed.drop(high_cor_pairs[0][0],axis=1)\ndf_drop","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"while high_cor_pairs != []:\n    high_cor_pairs = print_highly_correlated2(df=df_drop, features=df_drop.columns)\n    if high_cor_pairs == []:\n        break\n    df_drop = df_drop.drop(high_cor_pairs[0][0],axis=1)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Теперь в нашем датасете нет ни одной пары признаков корреляция спирмана для которых >=0.6","metadata":{}},{"cell_type":"code","source":"df_drop","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sp_corr_mat = df_drop.corr(method = 'spearman')\nfig = plt.figure(figsize=(5, 3))\nplt.title(\"Spearman Correlation Matrix\")\nmask = np.triu(np.ones_like(sp_corr_mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(sp_corr_mat,\n           mask = mask,\n           cmap=\"Blues\", annot=True, center=0, fmt='.2f')\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Как видим теперь максимум на графике соответсвует 0.6, чего мы и хотели добиться :)\nцифра 0.6 появилась постфактум, потому что изначально я брал значение в 0.85, но разница между моделями была еще меньше..","metadata":{}},{"cell_type":"markdown","source":"Таким образом мы понизили размерность с 200+ до 112","metadata":{}},{"cell_type":"markdown","source":"# Battle двух датасетов:","metadata":{}},{"cell_type":"markdown","source":"Дата сет со всеми посчитанными дескрипторами RDKit (**df1**) vs Очищенный от лишних фичей (по критерию корреляции Спирмана) (**df2**) ","metadata":{}},{"cell_type":"code","source":"df1 = data_imputed\ndf1","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2 = data_imputed[df_drop.columns]\ndf2","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df1.drop(['Target Name'],axis=1)\ny = df1['Target Name']\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=13)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_traindf2 = X_train[df_drop.drop(['Target Name'],axis=1).columns]\nX_testdf2 =  X_test[df_drop.drop(['Target Name'],axis=1).columns]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Проведем оптимизацию гиперпараметров для RFC для подготовиленного датасета\n","metadata":{}},{"cell_type":"markdown","source":"Сначала воспользуемся RandomizedSearchCV а потом уже GridSearchem","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier()\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\nmax_features = ['log2', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(start = 1, stop = 15, num = 15)]\nmin_samples_split = [int(x) for x in np.linspace(start = 2, stop = 50, num = 10)]\nmin_samples_leaf = [int(x) for x in np.linspace(start = 2, stop = 50, num = 10)]\nbootstrap = [True, False]\nparam_dist = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nrs = RandomizedSearchCV(rfc, \n                        param_dist, \n                        n_iter = 100, \n                        cv = 5, \n                        verbose = 1, \n                        n_jobs=-1, \n                        random_state=0)\nrs.fit(X_traindf2, y_train)\nrs.best_params_","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"После чего строю графики для каждого параметра, где можно будет визуально увидеть лучшие значения","metadata":{}},{"cell_type":"code","source":"rs_df = pd.DataFrame(rs.cv_results_).sort_values('rank_test_score').reset_index(drop=True)\nrs_df = rs_df.drop([\n            'mean_fit_time', \n            'std_fit_time', \n            'mean_score_time',\n            'std_score_time', \n            'params', \n            'split0_test_score', \n            'split1_test_score', \n            'split2_test_score', \n            'std_test_score'],\n            axis=1)\nfig, axs = plt.subplots(ncols=3, nrows=2)\nsns.set(style=\"whitegrid\", color_codes=True, font_scale = 2)\nfig.set_size_inches(30,25)\nsns.barplot( data=rs_df,x='param_n_estimators', y='mean_test_score', ax=axs[0,0], color='lightgrey')\naxs[0,0].set_ylim([0,0.8])\naxs[0,0].set_title(label = 'n_estimators', size=30, weight='bold')\nsns.barplot(x='param_min_samples_split', y='mean_test_score', data=rs_df, ax=axs[0,1], color='coral')\naxs[0,1].set_ylim([0,0.8])\naxs[0,1].set_title(label = 'min_samples_split', size=30, weight='bold')\nsns.barplot(x='param_min_samples_leaf', y='mean_test_score', data=rs_df, ax=axs[0,2], color='lightgreen')\naxs[0,2].set_ylim([0,0.8])\naxs[0,2].set_title(label = 'min_samples_leaf', size=30, weight='bold')\nsns.barplot(x='param_max_features', y='mean_test_score', data=rs_df, ax=axs[1,0], color='wheat')\naxs[1,0].set_ylim([0,0.8])\naxs[1,0].set_title(label = 'max_features', size=30, weight='bold')\nsns.barplot(x='param_max_depth', y='mean_test_score', data=rs_df, ax=axs[1,1], color='lightpink')\naxs[1,1].set_ylim([0,0.8])\naxs[1,1].set_title(label = 'max_depth', size=30, weight='bold')\nsns.barplot(x='param_bootstrap',y='mean_test_score', data=rs_df, ax=axs[1,2], color='skyblue')\naxs[1,2].set_ylim([0,0.8])\naxs[1,2].set_title(label = 'bootstrap', size=30, weight='bold')\nplt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Далее уже из вручную выбранных значений, с помощью GridSearchCV находим наилучшую комбинацию гиперпараметров для нашей модели","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nn_estimators = [500,900]\nmin_samples_split = [2,23]\nmin_samples_leaf = [2,18]\nmax_features = ['sqrt']\nmax_depth = [7,12]\nbootstrap = [True,False]\nparam_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\ngs = GridSearchCV(rfc, param_grid, cv = 3, verbose = 1, n_jobs=-1)\ngs.fit(X_traindf2, y_train)\nrfc_3 = gs.best_estimator_\ngs.best_params_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = gs.best_estimator_.predict(X_testdf2)\nprint(classification_report(y_test, y_pred))","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Теперь посчитаем тоже самое для датасета без отфильтрованных фичей","metadata":{}},{"cell_type":"code","source":"rfc_3.fit(X_train,y_train)\ny_pred = rfc_3.predict(X_test)\nprint(classification_report(y_test, y_pred))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Разницы в выборе двух датасетов для нашей задачи классификации практически не оказалось, что отлично, ведь мы сократили размерность с 200+ до 112 с сохранением всех метрик!","metadata":{}},{"cell_type":"markdown","source":"# Попробуем понизить размерность с помощью PCA","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca_test = PCA(n_components=210)\npca_test.fit(X_train)\nsns.set(style='whitegrid')\nplt.plot(np.cumsum(pca_test.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.axvline(linewidth=4, color='r', linestyle = '--', x=53, ymin=0, ymax=1)\ndisplay(plt.show())\nevr = pca_test.explained_variance_ratio_\ncvr = np.cumsum(pca_test.explained_variance_ratio_)\npca_df = pd.DataFrame()\npca_df['Cumulative Variance Ratio'] = cvr\npca_df['Explained Variance Ratio'] = evr\ndisplay(pca_df.iloc[50:55])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Взяв 53 главных компонент мы объясняем 95% дисперсии данных, но для сравнения с нашим методом понижения размерности путем удаление высокоскоррелированных признаков, возьмем нашу конечную размерность = 112","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=112)\npca.fit(X_train)\nTrain_pca = pca.transform(X_train)\npca.fit(X_test)\nTest_pca = pca.transform(X_test)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rfc_3.fit(Train_pca,y_train)\ny_pred = gs.best_estimator_.predict(Test_pca)\nprint(classification_report(y_test, y_pred))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"И снова PCA показывает результаты ощутимо хуже по всем метрикам! То есть наш метод понижения размерности гораздо эффективнее!","metadata":{}},{"cell_type":"markdown","source":"# Выводы по задаче классификации:","metadata":{}},{"cell_type":"markdown","source":"Используя отпечатки моргана r = 2, битность = 2048 и **RFC** в качестве модели мы получаем:  \n-------------------------------**F1-score**  \nDopamine D1 receptor -          0.90      \nDopamine D2 receptor  -         0.79      \nDopamine D3 receptor   -      0.55       \nDopamine D4 receptor    -        0.83       \n ----------------------------**accuracy 0.76**       \n \nИспользуя те же отпечатки и **RFC** в качестве модели, но с предварительным понижением размерности с помощью **PCA** мы получаем:  \n-------------------------------**F1-score**  \nDopamine D1 receptor -          0.30      \nDopamine D2 receptor  -         0.67      \nDopamine D3 receptor   -      0.06       \nDopamine D4 receptor    -        0.00       \n ----------------------------**accuracy 0.52**      \n \nИспользуя те же отпечатки и **GBC** в качестве модели мы получаем:  \n-------------------------------**F1-score**  \nDopamine D1 receptor -          0.86      \nDopamine D2 receptor  -         0.74      \nDopamine D3 receptor   -      0.49       \nDopamine D4 receptor    -        0.85       \n ----------------------------**accuracy 0.72**      ","metadata":{}},{"cell_type":"markdown","source":"\nИспользуя все дескрипторы полученные в **RDKit** - модель **RFC**:  \n-------------------------------**F1-score**  \nDopamine D1 receptor - 0.86  \nDopamine D2 receptor - 0.76  \nDopamine D3 receptor - 0.56  \nDopamine D4 receptor - 0.83  \n----------------------------**accuracy 0.74**  \n\nИспользуя нескоррелированные дескрипторы полученные в **RDKit** - модель **RFC**:  \n-------------------------------**F1-score**  \nDopamine D1 receptor - 0.85  \nDopamine D2 receptor - 0.76  \nDopamine D3 receptor - 0.57  \nDopamine D4 receptor - 0.82  \n----------------------------**accuracy 0.74**  \n\nИспользуя PCA в качестве алгоритма понижения размерности дескрипторов в **RDKit** - модель **RFC**:  \n-------------------------------**F1-score**  \nDopamine D1 receptor - 0.45  \nDopamine D2 receptor - 0.69  \nDopamine D3 receptor - 0.20  \nDopamine D4 receptor - 0.06  \n----------------------------**accuracy 0.56**  ","metadata":{}},{"cell_type":"markdown","source":"Таким образом можно легко увидеть, что отпечатки моргана дают наилучший результат для предсказания класса лиганда дофаминового рецептора, исходя из чего можно сделать вывод, что химическая структура напрямую определяет класс лиганда, чего не было в случае решения задачи регрессии по предсказанию концентрации pEC50!   \n\nДескрипторы RDKit также дают схожую точность, наилучшим решением является использование не всех дескрипторов, а только те, которые не будут являться высокоскоррелированными друг с другом и моделью выбрать RFC\n\nЕще один простой вывод, PCA не волшебная вещь, которая не позволяет магическим образом понизить размерность без потери качества модели, особенно когда мы говорим о понижении размерности таких систем, как отпечатки моргана... Во всех примерах в данной работе использование PCA приводило к сущетсвенному ухудшению работы модели. В то время как метод понижения размености путем анализа корреляции Спирмана для всех признаков с последующим удалением высокоскоррелированных, позволяет не просто понизить размерность, но даже не потерять эффективности работы модели! (а в случае больших объемов сетов, я уверен, что эффективность модели даже вырастет!) Так что Data-Preproccecing - наше все!","metadata":{}},{"cell_type":"markdown","source":"# The End","metadata":{}}]}